{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDSTchLGlKbK"
   },
   "source": [
    "# Tobig's 정규세션 10주차 음성 과제 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9kYTwinlcw2"
   },
   "source": [
    "- 다음 5개의 질문에 **단답형**으로 답해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z4WnJw7lhnf"
   },
   "source": [
    "## Q1. 시간에 따른 소리의 진폭을 나타낸 그래프를 영어로 무엇이라고 하나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14GNeFpPl5Y4"
   },
   "source": [
    "A1. Waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4zQxujcl-SY"
   },
   "source": [
    "## Q2. 샘플링 주파수는 원래 신호의 최고 주파수의 2배 이상이 되어야 원래 신호로 복구할 수 있다는 정리의 이름을 영어로 무엇이라고 하나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTKEol3_l-U9"
   },
   "source": [
    "A2. Nyquist-Shannon Sampling Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7oHJrZdmQlx"
   },
   "source": [
    "## Q3. 소리를 시각화한 것으로, 단시간 푸리에 변환을 통해 시간, 주파수, 진폭 정보를 모두 담고 있는 것을 영어로 무엇이라고 하나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3r_CJxTmQnW"
   },
   "source": [
    "A3. Spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Rth8m6cl-XP"
   },
   "source": [
    "## Q4. 인간의 청각 시스템에 맞게 Mel-scale을 적용하여 주파수를 조절한 스펙트로그램을 영어로 무엇이라고 하나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e_82O4VmxV9"
   },
   "source": [
    "A4. Mel Spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HcK1vyamxYZ"
   },
   "source": [
    "## Q5. Speech 정규세션 강의자가 수업 시작할 때 5명을 랜덤으로 지목하여 질문하겠다고 했으나, 실제로는 그보다 적은 n명에게 질문하였습니다. n의 값은 무엇입니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XGpjY4wl-ZS"
   },
   "source": [
    "A5. 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8M8RTTiVnnVC"
   },
   "source": [
    "# Tobig's 정규세션 10주차 음성 과제 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQArErJ0nr9O"
   },
   "source": [
    "1. train 폴더 내의 2,000개 음원을 이용하여 음성 분류 모델을 만들어보세요.\n",
    "2. 음성 분류 모델을 이용하여 test 폴더 내의 300개 음원을 분류하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bVPq_AMl3rx"
   },
   "outputs": [],
   "source": [
    "import librosa \n",
    "import librosa.display as dsp\n",
    "from IPython.display import Audio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') #GPU 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd()\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "audio_files = glob.glob(os.path.join(train_dir, '*.wav'))\n",
    "test_files = glob.glob(os.path.join(test_dir, '*.wav'))\n",
    "\n",
    "labels = [int(os.path.basename(file)[-5]) for file in audio_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(files, sr=44100):\n",
    "    audios = []\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    for file in files:\n",
    "        filename = os.path.basename(file)\n",
    "        label = int(filename[-5]) \n",
    "        labels.append(label)\n",
    "        filenames.append(filename)\n",
    "\n",
    "        audio, _ = librosa.load(file, sr=sr)\n",
    "        audios.append(audio)\n",
    "\n",
    "    return np.array(audios), np.array(labels),filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataset(files):\n",
    "    audio_data, labels , filenames = load_audio(files)\n",
    "    dataset = pd.DataFrame(list(zip(audio_data, labels)), columns=['data', 'label'])\n",
    "    return dataset\n",
    "\n",
    "train_wav = train_dataset(audio_files)\n",
    "train_wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(files):\n",
    "    audio_test, _ , filenames = load_audio(files)\n",
    "    dataset = pd.DataFrame(list(zip(audio_test, filenames)), columns=['data', 'filename'])\n",
    "    return dataset\n",
    "\n",
    "test_wav = test_dataset(test_files)\n",
    "test_wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_wav.data)\n",
    "test_x = np.array(test_wav.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini(data):\n",
    "\n",
    "    mini = 9999999\n",
    "    for i in data:\n",
    "        if len(i) < mini:\n",
    "            mini = len(i)\n",
    "\n",
    "    return mini\n",
    "\n",
    "train_mini = get_mini(train_x)\n",
    "test_mini = get_mini(test_x)\n",
    "\n",
    "#음성들의 길이를 맞추기\n",
    "mini = np.min([train_mini, test_mini])\n",
    "\n",
    "print('가장 작은 길이 :', mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_length(data, d_mini):\n",
    "\n",
    "    result = []\n",
    "    for i in data:\n",
    "        result.append(i[:d_mini])\n",
    "    result = np.array(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "train_x = set_length(train_x, mini)\n",
    "test_x = set_length(test_x, mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train :', train_x.shape)\n",
    "print('test :', test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_features = librosa.feature.mfcc(y=train_x[0], sr=44100, n_mfcc=40)\n",
    "# extracted_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(data):\n",
    "    mfccs = []\n",
    "    for i in data:\n",
    "        extracted_features = librosa.feature.mfcc(y=i,\n",
    "                                              sr=44100,\n",
    "                                              n_mfcc=40)\n",
    "        mfccs.append(extracted_features)\n",
    "            \n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mfccs = preprocess_dataset(train_x)\n",
    "train_mfccs = np.array(train_mfccs)\n",
    "train_mfccs = train_mfccs.reshape(-1, train_mfccs.shape[1], train_mfccs.shape[2], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train_mfccs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y, train_mode=True, transforms=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.train_mode = train_mode\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            X = self.transforms(X)\n",
    "\n",
    "        if self.train_mode:\n",
    "            y = self.y[index]\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def __len__(self): #길이 return\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_mfccs[:1800]\n",
    "vali_X = train_mfccs[1800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_wav.label[:1800]\n",
    "vali_y = train_wav.label[1800:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = CustomDataset(X=train_X, y=train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "vali_dataset = CustomDataset(X=vali_X, y=vali_y)\n",
    "vali_loader = DataLoader(vali_dataset, batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = len(train_loader)\n",
    "vali_batches = len(vali_loader)\n",
    "\n",
    "print('/ total train batches :', train_batches)\n",
    "print('/ total valid batches :', vali_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn \n",
    "\n",
    "class CNNclassification(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNclassification, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "          nn.Conv2d(40, 10, kernel_size=2, stride=1, padding=1),\n",
    "          nn.BatchNorm2d(10),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.2),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "          nn.Conv2d(10, 100, kernel_size=2, stride=1, padding=1),\n",
    "          nn.BatchNorm2d(100),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.2),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "          nn.Conv2d(100, 200, kernel_size=2, stride=1, padding=1),\n",
    "          nn.BatchNorm2d(200),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.2),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "          nn.Conv2d(200, 300, kernel_size=2, stride=1, padding=1),\n",
    "          nn.BatchNorm2d(300),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.2),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "          nn.Linear(300, 10),\n",
    "          nn.Dropout(0.2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        out = self.fc_layer(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim # 최적화 알고리즘들이 포함힘\n",
    "\n",
    "model = CNNclassification().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-3,  weight_decay=1e-5)\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.rand(10, 40, 8, 1).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "data_dir = os.getcwd()\n",
    "model_save_path = os.path.join(data_dir, 'model/best_model.pth')\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "\n",
    "def train(model, optimizer, train_loader, scheduler, device): \n",
    "    model.to(device)\n",
    "    n = len(train_loader)\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(1,num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for wav, label in tqdm(iter(train_loader)):\n",
    "            \n",
    "            wav, label = wav.to(device), label.to(device) \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            logit = model(wav)\n",
    "            loss = criterion(logit, label) \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "             \n",
    "        print('[%d] Train loss: %.10f' %(epoch, running_loss / len(train_loader)))\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        model.eval()\n",
    "        vali_loss = 0.0\n",
    "        correct = 0\n",
    "       \n",
    "        with torch.no_grad(): \n",
    "            for wav, label in tqdm(iter(vali_loader)):\n",
    "                \n",
    "                wav, label = wav.to(device), label.to(device)\n",
    "                logit = model(wav)\n",
    "                vali_loss += criterion(logit, label)\n",
    "                pred = logit.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "        vali_acc = 100 * correct / len(vali_loader.dataset)\n",
    "        print('Vail set: Loss: {:.4f}, Accuracy: {}/{} ( {:.0f}%)\\n'.format(vali_loss / len(vali_loader), correct, len(vali_loader.dataset), 100 * correct / len(vali_loader.dataset)))\n",
    "        \n",
    "        if best_acc < vali_acc:\n",
    "            best_acc = vali_acc\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print('Model Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, train_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mfccs = preprocess_dataset(test_x)\n",
    "test_mfccs = np.array(test_mfccs)\n",
    "test_mfccs = test_mfccs.reshape(-1, test_mfccs.shape[1], test_mfccs.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, device):\n",
    "    model.eval()\n",
    "    model_pred = []\n",
    "    with torch.no_grad():\n",
    "        for wav in tqdm(iter(test_loader)):\n",
    "            wav = wav.to(device)\n",
    "\n",
    "            pred_logit = model(wav)\n",
    "            pred_logit = pred_logit.argmax(dim=1, keepdim=True).squeeze(1)\n",
    "\n",
    "            model_pred.extend(pred_logit.tolist())\n",
    "    return model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(X=test_mfccs, y= None, train_mode=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(model_save_path)\n",
    "model = CNNclassification().to(device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "preds = predict(model, test_loader, device)\n",
    "preds[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wav['label'] = preds\n",
    "test_wav = test_wav[['file_name', 'label']]\n",
    "\n",
    "pred_df = test_wav.copy()\n",
    "pred_df = pred_df.sort_values(by=[pred_df.columns[0]], ascending=[True]).reset_index(drop=True)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM2LkwdjPekujdc1Q17UV/k",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
