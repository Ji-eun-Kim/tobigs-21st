## 다양한 Learning Rate Scheduler 중 적어도 한 가지 소개하기 (A4 반 페이지 내외)
### Cyclical Learning Rates
<개념 및 동기>

Cyclical Learning Rates (CLR)는 Leslie N. Smith에 의해 소개된 개념으로, 학습률을 사이클 형태로 조정하는 방식입니다. 전통적인 학습률 스케줄러는 학습 과정에서 학습률을 점진적으로 감소시키지만, CLR은 최소값과 최대값 사이에서 학습률을 반복적으로 조정합니다. 이 접근법의 핵심 아이디어는 학습률을 증가시킴으로써 지역 최소값(local minima)에서 탈출하고, 다양한 스케일의 특성을 학습할 수 있게 하여 최적화 과정을 개선하는 것입니다.

<작동 원리>

CLR은 "triangular" 사이클을 기본으로 사용합니다. 이는 학습률이 선형적으로 증가한 후 선형적으로 감소하는 단순한 형태입니다. 사이클의 길이(step size)는 일반적으로 epoch 수의 2~10배로 설정되며, 이는 하나의 사이클이 완료되는데 필요한 iteration의 수를 의미합니다. 학습률의 상한과 하한은 실험적으로 결정되거나, "learning rate range test"를 통해 결정될 수 있습니다.

<장점>

지역 최소값 탈출: 학습률이 증가하는 구간에서 모델이 지역 최소값에서 탈출할 수 있게 도와줍니다.

하이퍼파라미터 조정 최소화: 전통적인 학습률 감소 방식에 비해, CLR을 사용하면 학습률과 같은 하이퍼파라미터의 수동 조정이 줄어듭니다.

빠른 수렴: 사이클을 통해 다양한 스케일의 특성을 학습할 수 있기 때문에, 모델이 더 빠르게 수렴할 수 있습니다.


## 강의에서 소개되지 않은 Training Error와 Generalization Error 사이 간극을 줄이는 방안 소개하기 (A4 반 페이지 내외)
### 조기 종료 (Early Stopping)
<개념>

조기 종료는 과적합을 방지하는 가장 직관적인 방법 중 하나입니다. 이 기법은 검증 세트에 대한 성능이 더 이상 개선되지 않을 때 학습을 중단시키는 전략입니다. 즉, 모델이 학습 데이터에 과도하게 적응하기 시작하고 일반화 성능이 떨어지기 전에 학습 과정을 멈춥니다.

<이점>

과적합 방지: 모델이 학습 데이터에 과적합되는 것을 방지합니다.

효율적인 학습: 불필요한 학습 시간을 줄여 학습 과정의 효율성을 높입니다.

최적화된 모델 선택: 검증 세트에 대한 성능이 최적인 시점의 모델을 선택할 수 있습니다.
