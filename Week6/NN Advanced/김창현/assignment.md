# 1. 다양한 Learning Rate Scheduler 중 적어도 한 가지 소개하기 (A4 반 페이지 내외)
---
## Cosine Annealing Learning Rate Scheduler
코사인 어닐링은 딥러닝에서 사용되는 학습률 스케줄링 기법 중 하나입니다.
이 기법은 초기에는 높은 학습률을 사용하여 빠르게 수렴하고, 학습이 진행됨에 따라 학습률을 점진적으로 감소시켜 모델이 미세 조정되도록 합니다.
이는 모델의 최적점을 탐색하며 학습을 안정화시키고 더 나은 일반화 성능을 달성하는 데 도움을 줍니다.
코사인 어닐링 스케줄링은 주로 네트워크가 최적의 해에 수렴할 때까지 학습률을 천천히 감소시키는 데 사용됩니다.
이를 위해 코사인 함수의 형태를 사용하여 학습률을 조정합니다. 코사인 함수는 주기적으로 진동하며 최솟값과 최댓값을 가지는 특징이 있습니다.
주어진 최대 에폭(T_max) 동안 학습률은 코사인 함수 모양으로 변경됩니다.
학습률은 초기 학습률에서 시작하여 주기적으로 코사인 함수를 따라 내려가며, 특정 주기에 따라 최솟값인 최소 학습률(eta_min)에 도달합니다. 
이후 학습률은 다시 증가하기 시작하여 주기를 반복합니다.
이러한 방식으로 학습률을 조절하면 초기에는 빠르게 수렴하면서도, 학습이 진행됨에 따라 안정적으로 모델을 미세 조정할 수 있습니다. 또한 코사인 어닐링은 학습률을 조정하는 데 있어서 주기적이고 예측 가능한 패턴을 제공하여 하이퍼파라미터 튜닝에 도움이 됩니다. 이는 딥러닝 모델의 성능 향상과 수렴 속도를 높일 수 있습니다.

# 2. 강의에서 소개되지 않은 Training Error와 Generalization Error 사이 간극을 줄이는 방안 소개하기
--- 
1. 앙상블 학습(Ensemble Learning):
앙상블 학습은 여러 다른 모델을 결합하여 하나의 모델보다 더 나은 성능을 달성하는 기법입니다.
서로 다른 모델을 함께 사용함으로써 각 모델이 학습한 다양한 특징을 결합하여 보다 강력한 일반화 성능을 달성할 수 있습니다.
예를 들어, 랜덤 포레스트나 그래디언트 부스팅 트리와 같은 앙상블 모델을 사용하여 Train error와 Generalization error 간의 간격을 줄일 수 있습니다.

2. 데이터 증강(Data Augmentation):
데이터 증강은 원래 데이터를 변형하여 새로운 학습 예제를 생성하는 기법입니다.
 이를 통해 모델은 더 많은 다양성을 갖춘 데이터셋에서 학습하게 되어 일반화 능력이 향상될 수 있습니다.
 예를 들어, 이미지 분류 작업에서 이미지를 회전, 이동, 반전시키거나 작은 변화를 주어 새로운 학습 데이터를 생성할 수 있습니다.

4. 특징 선택(Feature Selection):
특징 선택은 모델 학습에 사용되는 특징(변수)을 선택하는 과정입니다.
 불필요한 특징이나 중복되는 특징을 제거함으로써 모델의 복잡성을 줄이고 일반화 성능을 향상시킬 수 있습니다.
 특징 선택은 머신러닝 모델에 과적합을 방지하고 일반화 오차를 줄이는 데 도움이 될 수 있습니다.
 주요 특징을 선택하고 노이즈를 제거함으로써 모델의 복잡성을 줄이고, 따라서 일반화 오차를 줄일 수 있습니다.
